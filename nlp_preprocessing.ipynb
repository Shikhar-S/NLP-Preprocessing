{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic\n",
    "\n",
    "1. Cleaning text\n",
    "    - numbers\n",
    "    - punctuation\n",
    "    - whitespace\n",
    "    - accented characters\n",
    "    - case conversion\n",
    "    - abbreviations\n",
    "2. Tokenization\n",
    "3. Removing stopwords\n",
    "4. Stemming/Lemmatization\n",
    "5. Removing sparse terms/specific words.\n",
    "\n",
    "## Advanced\n",
    "1. Removing URL/html tags\n",
    "2. Cleaning and expanding emoticons.\n",
    "3. POS Tagging\n",
    "4. Chunking\n",
    "5. NER Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH='./data/text.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading text from files\n",
    "- open(filename,mode) : Returns a file object for `filename` in `mode`.\n",
    "- f.readlines() : Returns all lines in f as a list.\n",
    "- f.readline() : Returns single line from f.\n",
    "- f.read(size=-1) : Returns `size` bytes from f. \n",
    "\n",
    "Reference : https://docs.python.org/3/tutorial/inputoutput.html \n",
    "\n",
    "Reference : https://docs.python.org/3/tutorial/inputoutput.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "﻿The Project Gutenberg EBook of Crime and Punishme\n"
    }
   ],
   "source": [
    "with open(DATA_PATH,'r') as F:\n",
    "    text=F.read()\n",
    "    print(text[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the file is too large to fit in memory, one way is to read using for loop and process text line by line as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Expressions\n",
    "Using `re` package in python. Important functions:\n",
    "- re.match(pattern,text) : Matches `pattern` to beginning of `text`. Returns `match` object. \n",
    "\n",
    "- re.search(pattern,text) : Matches `pattern` to first occurrence in `text`. Returns `match` object.\n",
    "\n",
    "- re.findall(pattern,text) : Finds all non-overlapping occurrences of `pattern` in `text`. Returns python list.\n",
    "\n",
    "- re.sub(pattern,replacement,text) : Substitutes all leftmost non-overlapping occurrences of pattern in text by replacement. Returns replaced string. Replacement can be a string or a function. If it is a function, it takes a single `match` object as input and returns string.\n",
    "\n",
    "- re.split(pattern,text)  : Splits `text` with `pattern`. Returns list of strings.\n",
    "\n",
    "Reference : https://docs.python.org/3/library/re.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Data\n",
    "#### Handling Numbers\n",
    "- Using inflect \n",
    "\n",
    "Reference : https://pypi.org/project/inflect/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "There are 12 items in a dozen, and 20 in a score.\nThere are  items in a dozen, and  in a score.\n"
    }
   ],
   "source": [
    "# Removing numbers using re\n",
    "import re\n",
    "text= \"There are 12 items in a dozen, and 20 in a score.\"\n",
    "modified_text = re.sub(r'\\d+','',text)\n",
    "\n",
    "print(text)\n",
    "print(modified_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "There are 12 items in a dozen, and 20 in a score.\nThere are twelve items in a dozen, and twenty in a score.\n"
    }
   ],
   "source": [
    "\n",
    "# Replacing numbers using inflect and re\n",
    "import inflect\n",
    "import re\n",
    "\n",
    "text= \"There are 12 items in a dozen, and 20 in a score.\"\n",
    "inflect_engine = inflect.engine()\n",
    "\n",
    "def replace_number(match_obj):\n",
    "    matched_string = match_obj.group()\n",
    "    return inflect_engine.number_to_words(matched_string)\n",
    "\n",
    "modified_text = re.sub(r'\\d+',replace_number,text)\n",
    "\n",
    "print(text)\n",
    "print(modified_text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Punctuation\n",
    "- str.maketrans(x,y,z) : Returns a translation table to map chars in `x` to corresponding chars in `y` and chars in `z` to `None`.\n",
    "- string.punctuation : Constant string containing all punctuation symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Python default punctuation:  !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\nSome [text], {;with? ra@ndom punctuation !\nSome text with random punctuation \n"
    }
   ],
   "source": [
    "print('Python default punctuation: ', string.punctuation)\n",
    "\n",
    "text = \"Some [text], {;with? ra@ndom punctuation !\"\n",
    "\n",
    "translator = str.maketrans('','',string.punctuation)\n",
    "modified_text = text.translate(translator)\n",
    "\n",
    "print(text)\n",
    "print(modified_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n   \t Some text with extra whitespace   \nSome text with extra whitespace\n"
    }
   ],
   "source": [
    "text = \"   \\t Some text with extra whitespace   \"\n",
    "modified_text = text.strip()\n",
    "\n",
    "print('')\n",
    "print(text)\n",
    "print(modified_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Accented Characters\n",
    "- Using unidecode\n",
    "\n",
    "Reference : https://pypi.org/project/Unidecode/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Would you like to have latté at our café?\nWould you like to have latte at our cafe?\n"
    }
   ],
   "source": [
    "import unidecode\n",
    "\n",
    "text = \"Would you like to have latté at our café?\"\n",
    "modified_text = unidecode.unidecode(text)\n",
    "\n",
    "print(text)\n",
    "print(modified_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Case Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Text having bOth SmAll aNd CapItal LetterS.\ntext having both small and capital letters.\nTEXT HAVING BOTH SMALL AND CAPITAL LETTERS.\n"
    }
   ],
   "source": [
    "text = 'Text having bOth SmAll aNd CapItal LetterS.'\n",
    "modified_text_small = text.lower()\n",
    "modified_text_capital = text.upper()\n",
    "\n",
    "print(text)\n",
    "print(modified_text_small)\n",
    "print(modified_text_capital)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Abbreviations\n",
    "- Using spacy. Note that this is not perfect. Ex, didn't -> do not, I'm -> I be, gonna -> go to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Oh no he didn't. I can't and I won't. I'll know what I'm gonna do.\noh no he do not . I can not and I will not . I will know what I be go to do .\n"
    }
   ],
   "source": [
    "import spacy\n",
    "# run python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "text= u\"Oh no he didn't. I can't and I won't. I'll know what I'm gonna do.\"\n",
    "doc = nlp(text)\n",
    "modified_text = \" \".join([token.lemma_ if token.pos_ is not 'PRON' else token.text for token in doc])\n",
    "\n",
    "print(text)\n",
    "print(modified_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "- str.split(separator=' ') : Returns list of tokens separated by `separator` in `str`.\n",
    "- nltk : `word_tokenizer` and `sentence_tokenizer` in nltk.\n",
    "- spacy : by creating a doc from `en` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Tokens : ['\\ufeffThe', 'Project', 'Gutenberg', 'EBook', 'of', 'Crime', 'and', 'Punishment,', 'by', 'Fyodor', 'Dostoevsky', 'This', 'eBook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever.', 'You', 'may', 'copy', 'it,', 'give', 'it', 'away', 'or', 're-use', 'it', 'under', 'the', 'terms', 'of', 'the', 'Project', 'Gutenberg', 'License', 'included', 'with', 'this']\nNumber of tokens :  206530\n"
    }
   ],
   "source": [
    "# Using python's split\n",
    "with open(DATA_PATH,'r') as F:\n",
    "    text = F.read()\n",
    "    tokens = text.split()\n",
    "    \n",
    "    print(\"Tokens : \",end='')\n",
    "    print(tokens[:50])\n",
    "    print(\"Number of tokens : \",len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['\\ufeffThe', 'Project', 'Gutenberg', 'EBook', 'of', 'Crime', 'and', 'Punishment', ',', 'by', 'Fyodor', 'Dostoevsky', 'This', 'eBook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', '.', 'You', 'may', 'copy', 'it', ',', 'give', 'it', 'away', 'or', 're-use', 'it', 'under', 'the', 'terms', 'of', 'the', 'Project', 'Gutenberg', 'License']\n"
    }
   ],
   "source": [
    "# Using nltk\n",
    "# run python -m nltk.downloader all\n",
    "with open(DATA_PATH,'r') as F:\n",
    "    from nltk import word_tokenize\n",
    "    text = F.read()\n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "    print(tokens[:50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[﻿The, Project, Gutenberg, EBook, of, Crime, and, Punishment, ,, by, Fyodor, Dostoevsky, \n\n, This, eBook, is, for, the, use, of, anyone, anywhere, at, no, cost, and, with, \n, almost, no, restrictions, whatsoever, .,  , You, may, copy, it, ,, give, it, away, or, \n, re, -, use, it, under, the]\n"
    }
   ],
   "source": [
    "# Using spacy\n",
    "import spacy\n",
    "# run python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "with open(DATA_PATH,'r') as F:\n",
    "    text = F.read()\n",
    "    #creating spacy doc of first 10k characters. default max limit is 1000000\n",
    "    doc = nlp(text[:10000])\n",
    "    \n",
    "    #first 50 tokens\n",
    "    tokens = [ token for token in doc[:50]]\n",
    "    print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stopwords\n",
    "- nltk : Provides a set of English stopwords to filter.\n",
    "##### Use following imports for spacy/sklearn\n",
    "- spacy : `from spacy.lang.en.stop_words import STOP_WORDS` \n",
    "- sklearn : `from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS` \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['NLTK', 'leading', 'platform', 'building', 'Python', 'programs', 'work', 'human', 'language', 'data', '.']\n"
    }
   ],
   "source": [
    "# Using nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "text = 'NLTK is a leading platform for building Python programs to work with human language data.'\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "result = [i for i in tokens if not i in stop_words]\n",
    "print (result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and Lemmatization\n",
    "- nltk : Provides stemmers \n",
    "- spacy : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional References\n",
    "- https://stackabuse.com/python-for-nlp-tokenization-stemming-and-lemmatization-with-spacy-library/\n",
    "- https://spacy.io/\n",
    "- https://machinelearningmastery.com/clean-text-machine-learning-python/\n",
    "- https://www.kaggle.com/sudalairajkumar/getting-started-with-text-preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "﻿The Project Gutenberg EBook of Crime and Punishment, by Fyodor Dostoevsky\n\n"
    }
   ],
   "source": [
    "with open(DATA_PATH,'r') as F:\n",
    "    for line in F:\n",
    "        print(line)\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1593243584140",
   "display_name": "Python 3.7.3 64-bit ('anaconda3': virtualenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}