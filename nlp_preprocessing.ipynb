{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics covered\n",
    "### Basic\n",
    "1. Cleaning text\n",
    "    - numbers\n",
    "    - punctuation\n",
    "    - whitespace\n",
    "    - accented characters\n",
    "    - case conversion\n",
    "    - abbreviations\n",
    "2. Tokenization\n",
    "3. Removing stopwords\n",
    "4. Stemming/Lemmatization\n",
    "\n",
    "### Advanced\n",
    "5. Removing URL/html tags\n",
    "6. Cleaning and expanding emoticons.\n",
    "7. POS Tagging\n",
    "8. Chunking\n",
    "9. NER Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH='./data/text.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading text from files\n",
    "- open(filename,mode) : Returns a file object for `filename` in `mode`.\n",
    "- f.readlines() : Returns all lines in f as a list.\n",
    "- f.readline() : Returns single line from f.\n",
    "- f.read(size=-1) : Returns `size` bytes from f. \n",
    "\n",
    "Reference : https://docs.python.org/3/tutorial/inputoutput.html \n",
    "\n",
    "Reference : https://docs.python.org/3/tutorial/inputoutput.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "ï»¿The Project Gutenberg EBook of Crime and Punishme\n"
    }
   ],
   "source": [
    "with open(DATA_PATH,'r') as F:\n",
    "    text=F.read()\n",
    "    print(text[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the file is too large to fit in memory, one way is to read using for loop and process text line by line as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Expressions\n",
    "Using `re` package in python. Important functions:\n",
    "- re.match(pattern,text) : Matches `pattern` to beginning of `text`. Returns `match` object. \n",
    "\n",
    "- re.search(pattern,text) : Matches `pattern` to first occurrence in `text`. Returns `match` object.\n",
    "\n",
    "- re.findall(pattern,text) : Finds all non-overlapping occurrences of `pattern` in `text`. Returns python list.\n",
    "\n",
    "- re.sub(pattern,replacement,text) : Substitutes all leftmost non-overlapping occurrences of pattern in text by replacement. Returns replaced string. Replacement can be a string or a function. If it is a function, it takes a single `match` object as input and returns string.\n",
    "\n",
    "- re.split(pattern,text)  : Splits `text` with `pattern`. Returns list of strings.\n",
    "\n",
    "Reference : https://docs.python.org/3/library/re.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Data\n",
    "#### Handling Numbers\n",
    "- Using inflect \n",
    "\n",
    "Reference : https://pypi.org/project/inflect/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "There are 12 items in a dozen, and 20 in a score.\nThere are  items in a dozen, and  in a score.\n"
    }
   ],
   "source": [
    "# Removing numbers using re\n",
    "import re\n",
    "text= \"There are 12 items in a dozen, and 20 in a score.\"\n",
    "modified_text = re.sub(r'\\d+','',text)\n",
    "\n",
    "print(text)\n",
    "print(modified_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "There are 12 items in a dozen, and 20 in a score.\nThere are twelve items in a dozen, and twenty in a score.\n"
    }
   ],
   "source": [
    "\n",
    "# Replacing numbers using inflect and re\n",
    "import inflect\n",
    "import re\n",
    "\n",
    "text= \"There are 12 items in a dozen, and 20 in a score.\"\n",
    "inflect_engine = inflect.engine()\n",
    "\n",
    "def replace_number(match_obj):\n",
    "    matched_string = match_obj.group()\n",
    "    return inflect_engine.number_to_words(matched_string)\n",
    "\n",
    "modified_text = re.sub(r'\\d+',replace_number,text)\n",
    "\n",
    "print(text)\n",
    "print(modified_text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Punctuation\n",
    "- str.maketrans(x,y,z) : Returns a translation table to map chars in `x` to corresponding chars in `y` and chars in `z` to `None`.\n",
    "- string.punctuation : Constant string containing all punctuation symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Python default punctuation:  !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\nSome [text], {;with? ra@ndom punctuation !\nSome text with random punctuation \n"
    }
   ],
   "source": [
    "print('Python default punctuation: ', string.punctuation)\n",
    "\n",
    "text = \"Some [text], {;with? ra@ndom punctuation !\"\n",
    "\n",
    "translator = str.maketrans('','',string.punctuation)\n",
    "modified_text = text.translate(translator)\n",
    "\n",
    "print(text)\n",
    "print(modified_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n   \t Some text with extra whitespace   \nSome text with extra whitespace\n"
    }
   ],
   "source": [
    "text = \"   \\t Some text with extra whitespace   \"\n",
    "modified_text = text.strip()\n",
    "\n",
    "print('')\n",
    "print(text)\n",
    "print(modified_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Accented Characters\n",
    "- Using unidecode\n",
    "\n",
    "Reference : https://pypi.org/project/Unidecode/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Would you like to have lattÃ© at our cafÃ©?\nWould you like to have latte at our cafe?\n"
    }
   ],
   "source": [
    "import unidecode\n",
    "\n",
    "text = \"Would you like to have lattÃ© at our cafÃ©?\"\n",
    "modified_text = unidecode.unidecode(text)\n",
    "\n",
    "print(text)\n",
    "print(modified_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Case Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Text having bOth SmAll aNd CapItal LetterS.\ntext having both small and capital letters.\nTEXT HAVING BOTH SMALL AND CAPITAL LETTERS.\n"
    }
   ],
   "source": [
    "text = 'Text having bOth SmAll aNd CapItal LetterS.'\n",
    "modified_text_small = text.lower()\n",
    "modified_text_capital = text.upper()\n",
    "\n",
    "print(text)\n",
    "print(modified_text_small)\n",
    "print(modified_text_capital)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Abbreviations\n",
    "- Using spacy. Note that this is not perfect. Ex, didn't -> do not, I'm -> I be, gonna -> go to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Oh no he didn't. I can't and I won't. I'll know what I'm gonna do.\noh no he do not . I can not and I will not . I will know what I be go to do .\n"
    }
   ],
   "source": [
    "import spacy\n",
    "# run python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "text= u\"Oh no he didn't. I can't and I won't. I'll know what I'm gonna do.\"\n",
    "doc = nlp(text)\n",
    "modified_text = \" \".join([token.lemma_ if token.pos_ is not 'PRON' else token.text for token in doc])\n",
    "\n",
    "print(text)\n",
    "print(modified_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "- str.split(separator=' ') : Returns list of tokens separated by `separator` in `str`.\n",
    "- nltk : `word_tokenizer` and `sentence_tokenizer` in nltk.\n",
    "- spacy : by creating a doc from `en` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Tokens : ['\\ufeffThe', 'Project', 'Gutenberg', 'EBook', 'of', 'Crime', 'and', 'Punishment,', 'by', 'Fyodor', 'Dostoevsky', 'This', 'eBook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever.', 'You', 'may', 'copy', 'it,', 'give', 'it', 'away', 'or', 're-use', 'it', 'under', 'the', 'terms', 'of', 'the', 'Project', 'Gutenberg', 'License', 'included', 'with', 'this']\nNumber of tokens :  206530\n"
    }
   ],
   "source": [
    "# Using python's split\n",
    "with open(DATA_PATH,'r') as F:\n",
    "    text = F.read()\n",
    "    tokens = text.split()\n",
    "    \n",
    "    print(\"Tokens : \",end='')\n",
    "    print(tokens[:50])\n",
    "    print(\"Number of tokens : \",len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['\\ufeffThe', 'Project', 'Gutenberg', 'EBook', 'of', 'Crime', 'and', 'Punishment', ',', 'by', 'Fyodor', 'Dostoevsky', 'This', 'eBook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', '.', 'You', 'may', 'copy', 'it', ',', 'give', 'it', 'away', 'or', 're-use', 'it', 'under', 'the', 'terms', 'of', 'the', 'Project', 'Gutenberg', 'License']\n"
    }
   ],
   "source": [
    "# Using nltk\n",
    "# run python -m nltk.downloader all\n",
    "with open(DATA_PATH,'r') as F:\n",
    "    from nltk import word_tokenize\n",
    "    text = F.read()\n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "    print(tokens[:50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[ï»¿The, Project, Gutenberg, EBook, of, Crime, and, Punishment, ,, by, Fyodor, Dostoevsky, \n\n, This, eBook, is, for, the, use, of, anyone, anywhere, at, no, cost, and, with, \n, almost, no, restrictions, whatsoever, .,  , You, may, copy, it, ,, give, it, away, or, \n, re, -, use, it, under, the]\n"
    }
   ],
   "source": [
    "# Using spacy\n",
    "import spacy\n",
    "# run python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "with open(DATA_PATH,'r') as F:\n",
    "    text = F.read()\n",
    "    #creating spacy doc of first 10k characters. default max limit is 1000000\n",
    "    doc = nlp(text[:10000])\n",
    "    \n",
    "    #first 50 tokens\n",
    "    tokens = [ token for token in doc[:50]]\n",
    "    print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stopwords\n",
    "- nltk : Provides a set of English stopwords to filter.\n",
    "##### Use following imports for spacy/sklearn\n",
    "- spacy : `from spacy.lang.en.stop_words import STOP_WORDS` \n",
    "- sklearn : `from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS` \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['NLTK', 'leading', 'platform', 'building', 'Python', 'programs', 'work', 'human', 'language', 'data', '.']\n"
    }
   ],
   "source": [
    "# Using nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "text = 'NLTK is a leading platform for building Python programs to work with human language data.'\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "result = [i for i in tokens if not i in stop_words]\n",
    "print (result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specific words can be removed by adding to these stopword sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and Lemmatization\n",
    "- Stemming - the root may not be a dictionary word. Fast.\n",
    "- Lemmatization - root is always a dictionary word. Slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Porter Stemmer\ncompute --> comput\ncomputer --> comput\ncomputed --> comput\ncomputing --> comput\ncomputers --> comput\n"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "tokens = ['compute', 'computer', 'computed', 'computing','computers']\n",
    "\n",
    "print(\"Porter Stemmer\")\n",
    "for token in tokens:\n",
    "    print(token + ' --> ' + stemmer.stem(token))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "- from nltk.stem.snowball import SnowballStemmer\n",
    "- stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "uses SnowballStemmer which is an improvement over Porter Stemmer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Lemmatization\ncompute --> compute\ncomputer --> computer\ncomputed --> compute\ncomputing --> compute\ncomputers --> computer\n"
    }
   ],
   "source": [
    "# Using spacy\n",
    "import spacy\n",
    "# run python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "tokens = ['compute', 'computer', 'computed', 'computing','computers']\n",
    "doc=nlp(\" \".join(tokens))\n",
    "\n",
    "print(\"Lemmatization\")\n",
    "for token in doc:\n",
    "    print(token,'-->',token.lemma_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- from nltk.stem import WordNetLemmatizer \n",
    "- lemmatizer = WordNetLemmatizer() \n",
    "- lemmatizer.lemmatize(word, pos ='v') # pos is part of speech tag\n",
    "\n",
    "can be used for lemmatization with nltk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing URLs and html tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Driverless AI NLP blog post on https://www.h2o.ai/blog/detecting-sarcasm-is-difficult-but-ai-may-have-an-answer/\nDriverless AI NLP blog post on \n"
    }
   ],
   "source": [
    "# Removing URL from data using regex\n",
    "\n",
    "text = \"Driverless AI NLP blog post on https://www.h2o.ai/blog/detecting-sarcasm-is-difficult-but-ai-may-have-an-answer/\"\n",
    "modified_text = re.sub(r'https?://\\S+|www\\.\\S+','',text)\n",
    "\n",
    "print(text)\n",
    "print(modified_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<div>\n<h1> H2O</h1>\n<p> AutoML</p>\n<a href=\"https://www.h2o.ai/products/h2o-driverless-ai/\"> Driverless AI</a>\n</div>\n\n H2O\n AutoML\n Driverless AI\n\n"
    }
   ],
   "source": [
    "# Removing html tags using regex\n",
    "\n",
    "text = \"\"\"<div>\n",
    "<h1> H2O</h1>\n",
    "<p> AutoML</p>\n",
    "<a href=\"https://www.h2o.ai/products/h2o-driverless-ai/\"> Driverless AI</a>\n",
    "</div>\"\"\"\n",
    "\n",
    "modified_text = re.sub(r'<.*?>',\"\",text)\n",
    "\n",
    "print(text)\n",
    "print(modified_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "- from bs4 import BeautifulSoup\n",
    "- BeautifulSoup(text, \"lxml\").text\n",
    "\n",
    "can also be used to get same result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling emotiocns and emoji in data\n",
    "- Emoticons :  :) :D \n",
    "- Emoji :  ðŸ”¥\n",
    "\n",
    "Reference : https://github.com/NeelShah18/emot/blob/master/emot/emo_unicode.py \n",
    "A useful emoticon and emoji dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'game is on '"
     },
     "metadata": {},
     "execution_count": 132
    }
   ],
   "source": [
    "# Reference : https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\n",
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)\n",
    "\n",
    "remove_emoji(\"game is on ðŸ”¥ðŸ”¥\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'Hello '"
     },
     "metadata": {},
     "execution_count": 151
    }
   ],
   "source": [
    "# For illsutration we are hardcoding values from dictionary referenced above.\n",
    "EMOTICONS = {\n",
    "    u\":-\\)\":\"Happy face or smiley\"}\n",
    "\n",
    "def remove_emoticons(text):\n",
    "    emoticon_pattern = re.compile(u'(' + u'|'.join(k for k in EMOTICONS) + u')')\n",
    "    return emoticon_pattern.sub(r'', text)\n",
    "\n",
    "remove_emoticons(\"Hello :-)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'Hello Happy_face_or_smiley Happy_face_or_smiley'"
     },
     "metadata": {},
     "execution_count": 153
    }
   ],
   "source": [
    "# Converting emoticons to words\n",
    "\n",
    "# For illsutration we are hardcoding values from dictionary referenced above.\n",
    "EMOTICONS = {\n",
    "    u\":-\\)\":\"Happy face or smiley\"}\n",
    "\n",
    "def convert_emoticons(text):\n",
    "    for emot in EMOTICONS:\n",
    "        text = re.sub(u'('+emot+')', \"_\".join(EMOTICONS[emot].replace(\",\",\"\").split()), text)\n",
    "    return text\n",
    "\n",
    "text = \"Hello :-) :-)\"\n",
    "convert_emoticons(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'game is on fire'"
     },
     "metadata": {},
     "execution_count": 156
    }
   ],
   "source": [
    "# Converting emoji to words\n",
    "\n",
    "# For illsutration we are hardcoding values from dictionary referenced above.\n",
    "UNICODE_EMO={\n",
    "    'ðŸ”¥' : 'fire'\n",
    "}\n",
    "def convert_emojis(text):\n",
    "    for emot in UNICODE_EMO:\n",
    "        text = re.sub(r'('+emot+')', \"_\".join(UNICODE_EMO[emot].replace(\",\",\"\").replace(\":\",\"\").split()), text)\n",
    "    return text\n",
    "\n",
    "text = \"game is on ðŸ”¥\"\n",
    "convert_emojis(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional References\n",
    "- https://stackabuse.com/python-for-nlp-tokenization-stemming-and-lemmatization-with-spacy-library/\n",
    "- https://spacy.io/\n",
    "- https://machinelearningmastery.com/clean-text-machine-learning-python/\n",
    "- https://www.kaggle.com/sudalairajkumar/getting-started-with-text-preprocessing \n",
    "- https://www.nltk.org/howto/stem.html\n",
    "- https://towardsdatascience.com/nlp-text-preprocessing-a-practical-guide-and-template-d80874676e79\n",
    "- https://medium.com/@datamonsters/text-preprocessing-in-python-steps-tools-and-examples-bf025f872908\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "ï»¿The Project Gutenberg EBook of Crime and Punishment, by Fyodor Dostoevsky\n\n"
    }
   ],
   "source": [
    "with open(DATA_PATH,'r') as F:\n",
    "    for line in F:\n",
    "        print(line)\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1593243584140",
   "display_name": "Python 3.7.3 64-bit ('anaconda3': virtualenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}