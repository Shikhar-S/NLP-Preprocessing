{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic\n",
    "\n",
    "1. Cleaning text\n",
    "    - numbers\n",
    "    - punctuation\n",
    "    - whitespace\n",
    "    - accented characters\n",
    "    - case conversion\n",
    "    - abbreviations\n",
    "2. Tokenization\n",
    "3. Removing stopwords\n",
    "4. Stemming/Lemmatization\n",
    "5. Removing sparse terms/specific words.\n",
    "\n",
    "## Advanced\n",
    "1. Removing URL/html tags\n",
    "2. Cleaning and expanding emoticons.\n",
    "3. POS Tagging\n",
    "4. Chunking\n",
    "5. NER Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH='./data/text.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading text from files\n",
    "- open(filename,mode) : Returns a file object for `filename` in `mode`.\n",
    "- f.readlines() : Returns all lines in f as a list.\n",
    "- f.readline() : Returns single line from f.\n",
    "- f.read(size=-1) : Returns `size` bytes from f. \n",
    "\n",
    "Reference : https://docs.python.org/3/tutorial/inputoutput.html \n",
    "\n",
    "Reference : https://docs.python.org/3/tutorial/inputoutput.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "﻿The Project Gutenberg EBook of Crime and Punishme\n"
    }
   ],
   "source": [
    "with open(DATA_PATH,'r') as F:\n",
    "    text=F.read()\n",
    "    print(text[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the file is too large to fit in memory, one way is to read using for loop and process text line by line as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Expressions\n",
    "Using `re` package in python. Important functions:\n",
    "- re.match(pattern,text) : Matches `pattern` to beginning of `text`. Returns `match` object. \n",
    "\n",
    "- re.search(pattern,text) : Matches `pattern` to first occurrence in `text`. Returns `match` object.\n",
    "\n",
    "- re.findall(pattern,text) : Finds all non-overlapping occurrences of `pattern` in `text`. Returns python list.\n",
    "\n",
    "- re.sub(pattern,replacement,text) : Substitutes all leftmost non-overlapping occurrences of pattern in text by replacement. Returns replaced string. Replacement can be a string or a function. If it is a function, it takes a single `match` object as input and returns string.\n",
    "\n",
    "- re.split(pattern,text)  : Splits `text` with `pattern`. Returns list of strings.\n",
    "\n",
    "Reference : https://docs.python.org/3/library/re.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Data\n",
    "#### Handling Numbers\n",
    "- Using inflect \n",
    "\n",
    "Reference : https://pypi.org/project/inflect/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "There are 12 items in a dozen, and 20 in a score.\nThere are  items in a dozen, and  in a score.\n"
    }
   ],
   "source": [
    "# Removing numbers using re\n",
    "import re\n",
    "text= \"There are 12 items in a dozen, and 20 in a score.\"\n",
    "modified_text = re.sub(r'\\d+','',text)\n",
    "\n",
    "print(text)\n",
    "print(modified_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "There are 12 items in a dozen, and 20 in a score.\nThere are twelve items in a dozen, and twenty in a score.\n"
    }
   ],
   "source": [
    "\n",
    "# Replacing numbers using inflect and re\n",
    "import inflect\n",
    "import re\n",
    "\n",
    "text= \"There are 12 items in a dozen, and 20 in a score.\"\n",
    "inflect_engine = inflect.engine()\n",
    "\n",
    "def replace_number(match_obj):\n",
    "    matched_string = match_obj.group()\n",
    "    return inflect_engine.number_to_words(matched_string)\n",
    "\n",
    "modified_text = re.sub(r'\\d+',replace_number,text)\n",
    "\n",
    "print(text)\n",
    "print(modified_text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Punctuation\n",
    "- str.maketrans(x,y,z) : Returns a translation table to map chars in `x` to corresponding chars in `y` and chars in `z` to `None`.\n",
    "- string.punctuation : Constant string containing all punctuation symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Python default punctuation:  !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\nSome [text], {;with? ra@ndom punctuation !\nSome text with random punctuation \n"
    }
   ],
   "source": [
    "print('Python default punctuation: ', string.punctuation)\n",
    "\n",
    "text = \"Some [text], {;with? ra@ndom punctuation !\"\n",
    "\n",
    "translator = str.maketrans('','',string.punctuation)\n",
    "modified_text = text.translate(translator)\n",
    "\n",
    "print(text)\n",
    "print(modified_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n   \t Some text with extra whitespace   \nSome text with extra whitespace\n"
    }
   ],
   "source": [
    "text = \"   \\t Some text with extra whitespace   \"\n",
    "modified_text = text.strip()\n",
    "\n",
    "print('')\n",
    "print(text)\n",
    "print(modified_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Accented Characters\n",
    "- Using unidecode\n",
    "\n",
    "Reference : https://pypi.org/project/Unidecode/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Would you like to have latté at our café?\nWould you like to have latte at our cafe?\n"
    }
   ],
   "source": [
    "import unidecode\n",
    "\n",
    "text = \"Would you like to have latté at our café?\"\n",
    "modified_text = unidecode.unidecode(text)\n",
    "\n",
    "print(text)\n",
    "print(modified_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Tokenization\n",
    "- str.split(separator=' ') : Returns list of tokens separated by `separator` in `str`.\n",
    "- nltk : word_tokenizer and sentence_tokenizer in nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Tokens : ['\\ufeffThe', 'Project', 'Gutenberg', 'EBook', 'of', 'Crime', 'and', 'Punishment,', 'by', 'Fyodor', 'Dostoevsky', 'This', 'eBook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever.', 'You', 'may', 'copy', 'it,', 'give', 'it', 'away', 'or', 're-use', 'it', 'under', 'the', 'terms', 'of', 'the', 'Project', 'Gutenberg', 'License', 'included', 'with', 'this']\nNumber of tokens :  206530\n"
    }
   ],
   "source": [
    "with open(DATA_PATH,'r') as F:\n",
    "    text=F.read()\n",
    "    tokens=text.split()\n",
    "    print(\"Tokens : \",end='')\n",
    "    print(tokens[:50])\n",
    "    print(\"Number of tokens : \",len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ImportError",
     "evalue": "cannot import name 'word_tokenizer' from 'nltk' (/mnt/c/Users/capta/wsl_home/anaconda3/lib/python3.7/site-packages/nltk/__init__.py)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-13b2bd178035>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mword_tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'word_tokenizer' from 'nltk' (/mnt/c/Users/capta/wsl_home/anaconda3/lib/python3.7/site-packages/nltk/__init__.py)"
     ]
    }
   ],
   "source": [
    "with open(DATA_PATH,'r') as F:\n",
    "    from nltk import word_tokenizer\n",
    "    text=F.read()\n",
    "    tokens=word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "﻿The Project Gutenberg EBook of Crime and Punishment, by Fyodor Dostoevsky\n\n"
    }
   ],
   "source": [
    "with open(DATA_PATH,'r') as F:\n",
    "    for line in F:\n",
    "        print(line)\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1593243584140",
   "display_name": "Python 3.7.3 64-bit ('anaconda3': virtualenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}