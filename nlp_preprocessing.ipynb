{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics covered\n",
    "### Basic\n",
    "1. Cleaning text\n",
    "    - numbers\n",
    "    - punctuation\n",
    "    - whitespace\n",
    "    - accented characters\n",
    "    - case conversion\n",
    "    - abbreviations\n",
    "2. Tokenization\n",
    "3. Removing stopwords\n",
    "4. Stemming/Lemmatization\n",
    "\n",
    "### Advanced\n",
    "5. Removing URL/html tags\n",
    "6. Cleaning and expanding emoticons.\n",
    "7. POS Tagging\n",
    "8. Chunking\n",
    "9. NER Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH='./data/text.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading text from files\n",
    "- open(filename,mode) : Returns a file object for `filename` in `mode`.\n",
    "- f.readlines() : Returns all lines in f as a list.\n",
    "- f.readline() : Returns single line from f.\n",
    "- f.read(size=-1) : Returns `size` bytes from f. \n",
    "\n",
    "Reference : https://docs.python.org/3/tutorial/inputoutput.html \n",
    "\n",
    "Reference : https://docs.python.org/3/tutorial/inputoutput.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿The Project Gutenberg EBook of Crime and Punishme\n"
     ]
    }
   ],
   "source": [
    "with open(DATA_PATH,'r') as F:\n",
    "    text=F.read()\n",
    "    print(text[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the file is too large to fit in memory, one way is to read using for loop and process text line by line as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Expressions\n",
    "Using `re` package in python. Important functions:\n",
    "- re.match(pattern,text) : Matches `pattern` to beginning of `text`. Returns `match` object. \n",
    "\n",
    "- re.search(pattern,text) : Matches `pattern` to first occurrence in `text`. Returns `match` object.\n",
    "\n",
    "- re.findall(pattern,text) : Finds all non-overlapping occurrences of `pattern` in `text`. Returns python list.\n",
    "\n",
    "- re.sub(pattern,replacement,text) : Substitutes all leftmost non-overlapping occurrences of pattern in text by replacement. Returns replaced string. Replacement can be a string or a function. If it is a function, it takes a single `match` object as input and returns string.\n",
    "\n",
    "- re.split(pattern,text)  : Splits `text` with `pattern`. Returns list of strings.\n",
    "\n",
    "Reference : https://docs.python.org/3/library/re.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Data\n",
    "#### Handling Numbers\n",
    "- Using inflect \n",
    "\n",
    "Reference : https://pypi.org/project/inflect/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 12 items in a dozen, and 20 in a score.\n",
      "There are  items in a dozen, and  in a score.\n"
     ]
    }
   ],
   "source": [
    "# Removing numbers using re\n",
    "import re\n",
    "text= \"There are 12 items in a dozen, and 20 in a score.\"\n",
    "modified_text = re.sub(r'\\d+','',text)\n",
    "\n",
    "print(text)\n",
    "print(modified_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 12 items in a dozen, and 20 in a score.\n",
      "There are twelve items in a dozen, and twenty in a score.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Replacing numbers using inflect and re\n",
    "import inflect\n",
    "import re\n",
    "\n",
    "text= \"There are 12 items in a dozen, and 20 in a score.\"\n",
    "inflect_engine = inflect.engine()\n",
    "\n",
    "def replace_number(match_obj):\n",
    "    matched_string = match_obj.group()\n",
    "    return inflect_engine.number_to_words(matched_string)\n",
    "\n",
    "modified_text = re.sub(r'\\d+',replace_number,text)\n",
    "\n",
    "print(text)\n",
    "print(modified_text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Punctuation\n",
    "- str.maketrans(x,y,z) : Returns a translation table to map chars in `x` to corresponding chars in `y` and chars in `z` to `None`.\n",
    "- string.punctuation : Constant string containing all punctuation symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python default punctuation:  !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "Some [text], {;with? ra@ndom punctuation !\n",
      "Some text with random punctuation \n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "print('Python default punctuation: ', string.punctuation)\n",
    "\n",
    "text = \"Some [text], {;with? ra@ndom punctuation !\"\n",
    "\n",
    "translator = str.maketrans('','',string.punctuation)\n",
    "modified_text = text.translate(translator)\n",
    "\n",
    "print(text)\n",
    "print(modified_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   \t Some text with extra whitespace   \n",
      "Some text with extra whitespace\n"
     ]
    }
   ],
   "source": [
    "text = \"   \\t Some text with extra whitespace   \"\n",
    "modified_text = text.strip()\n",
    "\n",
    "print('')\n",
    "print(text)\n",
    "print(modified_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Accented Characters\n",
    "- Using unidecode\n",
    "\n",
    "Reference : https://pypi.org/project/Unidecode/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Would you like to have latté at our café?\n",
      "Would you like to have latte at our cafe?\n"
     ]
    }
   ],
   "source": [
    "import unidecode\n",
    "\n",
    "text = \"Would you like to have latté at our café?\"\n",
    "modified_text = unidecode.unidecode(text)\n",
    "\n",
    "print(text)\n",
    "print(modified_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Case Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text having bOth SmAll aNd CapItal LetterS.\n",
      "text having both small and capital letters.\n",
      "TEXT HAVING BOTH SMALL AND CAPITAL LETTERS.\n"
     ]
    }
   ],
   "source": [
    "text = 'Text having bOth SmAll aNd CapItal LetterS.'\n",
    "modified_text_small = text.lower()\n",
    "modified_text_capital = text.upper()\n",
    "\n",
    "print(text)\n",
    "print(modified_text_small)\n",
    "print(modified_text_capital)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Abbreviations\n",
    "- Using spacy. Note that this is not perfect. Ex, didn't -> do not, I'm -> I be, gonna -> go to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh no he didn't. I can't and I won't. I'll know what I'm gonna do.\n",
      "oh no he do not . I can not and I will not . I will know what I be go to do .\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "# run python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "text= u\"Oh no he didn't. I can't and I won't. I'll know what I'm gonna do.\"\n",
    "doc = nlp(text)\n",
    "modified_text = \" \".join([token.lemma_ if token.pos_ is not 'PRON' else token.text for token in doc])\n",
    "\n",
    "print(text)\n",
    "print(modified_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "- str.split(separator=' ') : Returns list of tokens separated by `separator` in `str`.\n",
    "- nltk : `word_tokenizer` and `sentence_tokenizer` in nltk.\n",
    "- spacy : by creating a doc from `en` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens : ['\\ufeffThe', 'Project', 'Gutenberg', 'EBook', 'of', 'Crime', 'and', 'Punishment,', 'by', 'Fyodor', 'Dostoevsky', 'This', 'eBook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever.', 'You', 'may', 'copy', 'it,', 'give', 'it', 'away', 'or', 're-use', 'it', 'under', 'the', 'terms', 'of', 'the', 'Project', 'Gutenberg', 'License', 'included', 'with', 'this']\n",
      "Number of tokens :  206530\n"
     ]
    }
   ],
   "source": [
    "# Using python's split\n",
    "with open(DATA_PATH,'r') as F:\n",
    "    text = F.read()\n",
    "    tokens = text.split()\n",
    "    \n",
    "    print(\"Tokens : \",end='')\n",
    "    print(tokens[:50])\n",
    "    print(\"Number of tokens : \",len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\ufeffThe', 'Project', 'Gutenberg', 'EBook', 'of', 'Crime', 'and', 'Punishment', ',', 'by', 'Fyodor', 'Dostoevsky', 'This', 'eBook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', '.', 'You', 'may', 'copy', 'it', ',', 'give', 'it', 'away', 'or', 're-use', 'it', 'under', 'the', 'terms', 'of', 'the', 'Project', 'Gutenberg', 'License']\n"
     ]
    }
   ],
   "source": [
    "# Using nltk\n",
    "# run python -m nltk.downloader all\n",
    "with open(DATA_PATH,'r') as F:\n",
    "    from nltk import word_tokenize\n",
    "    text = F.read()\n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "    print(tokens[:50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[﻿The, Project, Gutenberg, EBook, of, Crime, and, Punishment, ,, by, Fyodor, Dostoevsky, \n",
      "\n",
      ", This, eBook, is, for, the, use, of, anyone, anywhere, at, no, cost, and, with, \n",
      ", almost, no, restrictions, whatsoever, .,  , You, may, copy, it, ,, give, it, away, or, \n",
      ", re, -, use, it, under, the]\n"
     ]
    }
   ],
   "source": [
    "# Using spacy\n",
    "import spacy\n",
    "# run python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "with open(DATA_PATH,'r') as F:\n",
    "    text = F.read()\n",
    "    #creating spacy doc of first 10k characters. default max limit is 1000000\n",
    "    doc = nlp(text[:10000])\n",
    "    \n",
    "    #first 50 tokens\n",
    "    tokens = [ token for token in doc[:50]]\n",
    "    print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stopwords\n",
    "- nltk : Provides a set of English stopwords to filter.\n",
    "##### Use following imports for spacy/sklearn\n",
    "- spacy : `from spacy.lang.en.stop_words import STOP_WORDS` \n",
    "- sklearn : `from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS` \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLTK', 'leading', 'platform', 'building', 'Python', 'programs', 'work', 'human', 'language', 'data', '.']\n"
     ]
    }
   ],
   "source": [
    "# Using nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "text = 'NLTK is a leading platform for building Python programs to work with human language data.'\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "result = [i for i in tokens if not i in stop_words]\n",
    "print (result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specific words can be removed by adding to these stopword sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and Lemmatization\n",
    "- Stemming - the root may not be a dictionary word. Fast.\n",
    "- Lemmatization - root is always a dictionary word. Slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter Stemmer\n",
      "compute --> comput\n",
      "computer --> comput\n",
      "computed --> comput\n",
      "computing --> comput\n",
      "computers --> comput\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "tokens = ['compute', 'computer', 'computed', 'computing','computers']\n",
    "\n",
    "print(\"Porter Stemmer\")\n",
    "for token in tokens:\n",
    "    print(token + ' --> ' + stemmer.stem(token))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "- from nltk.stem.snowball import SnowballStemmer\n",
    "- stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "uses SnowballStemmer which is an improvement over Porter Stemmer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization\n",
      "compute --> compute\n",
      "computer --> computer\n",
      "computed --> compute\n",
      "computing --> compute\n",
      "computers --> computer\n"
     ]
    }
   ],
   "source": [
    "# Using spacy\n",
    "import spacy\n",
    "# run python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "tokens = ['compute', 'computer', 'computed', 'computing','computers']\n",
    "doc=nlp(\" \".join(tokens))\n",
    "\n",
    "print(\"Lemmatization\")\n",
    "for token in doc:\n",
    "    print(token,'-->',token.lemma_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- from nltk.stem import WordNetLemmatizer \n",
    "- lemmatizer = WordNetLemmatizer() \n",
    "- lemmatizer.lemmatize(word, pos ='v') # pos is part of speech tag\n",
    "\n",
    "can be used for lemmatization with nltk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing URLs and html tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Driverless AI NLP blog post on https://www.h2o.ai/blog/detecting-sarcasm-is-difficult-but-ai-may-have-an-answer/\n",
      "Driverless AI NLP blog post on \n"
     ]
    }
   ],
   "source": [
    "# Removing URL from data using regex\n",
    "\n",
    "text = \"Driverless AI NLP blog post on https://www.h2o.ai/blog/detecting-sarcasm-is-difficult-but-ai-may-have-an-answer/\"\n",
    "modified_text = re.sub(r'https?://\\S+|www\\.\\S+','',text)\n",
    "\n",
    "print(text)\n",
    "print(modified_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div>\n",
      "<h1> H2O</h1>\n",
      "<p> AutoML</p>\n",
      "<a href=\"https://www.h2o.ai/products/h2o-driverless-ai/\"> Driverless AI</a>\n",
      "</div>\n",
      "\n",
      " H2O\n",
      " AutoML\n",
      " Driverless AI\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Removing html tags using regex\n",
    "\n",
    "text = \"\"\"<div>\n",
    "<h1> H2O</h1>\n",
    "<p> AutoML</p>\n",
    "<a href=\"https://www.h2o.ai/products/h2o-driverless-ai/\"> Driverless AI</a>\n",
    "</div>\"\"\"\n",
    "\n",
    "modified_text = re.sub(r'<.*?>',\"\",text)\n",
    "\n",
    "print(text)\n",
    "print(modified_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "- from bs4 import BeautifulSoup\n",
    "- BeautifulSoup(text, \"lxml\").text\n",
    "\n",
    "can also be used to get same result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling emotiocns and emoji in data\n",
    "- Emoticons :  :) :D \n",
    "- Emoji :  🔥\n",
    "\n",
    "Reference : https://github.com/NeelShah18/emot/blob/master/emot/emo_unicode.py \n",
    "A useful emoticon and emoji dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'game is on '"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reference : https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\n",
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)\n",
    "\n",
    "remove_emoji(\"game is on 🔥🔥\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For illsutration we are hardcoding values from dictionary referenced above.\n",
    "EMOTICONS = {\n",
    "    u\":-\\)\":\"Happy face or smiley\"}\n",
    "\n",
    "def remove_emoticons(text):\n",
    "    emoticon_pattern = re.compile(u'(' + u'|'.join(k for k in EMOTICONS) + u')')\n",
    "    return emoticon_pattern.sub(r'', text)\n",
    "\n",
    "remove_emoticons(\"Hello :-)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello Happy_face_or_smiley Happy_face_or_smiley'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting emoticons to words\n",
    "\n",
    "# For illsutration we are hardcoding values from dictionary referenced above.\n",
    "EMOTICONS = {\n",
    "    u\":-\\)\":\"Happy face or smiley\"}\n",
    "\n",
    "def convert_emoticons(text):\n",
    "    for emot in EMOTICONS:\n",
    "        text = re.sub(u'('+emot+')', \"_\".join(EMOTICONS[emot].replace(\",\",\"\").split()), text)\n",
    "    return text\n",
    "\n",
    "text = \"Hello :-) :-)\"\n",
    "convert_emoticons(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'game is on fire'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting emoji to words\n",
    "\n",
    "# For illsutration we are hardcoding values from dictionary referenced above.\n",
    "UNICODE_EMO={\n",
    "    '🔥' : 'fire'\n",
    "}\n",
    "def convert_emojis(text):\n",
    "    for emot in UNICODE_EMO:\n",
    "        text = re.sub(r'('+emot+')', \"_\".join(UNICODE_EMO[emot].replace(\",\",\"\").replace(\":\",\"\").split()), text)\n",
    "    return text\n",
    "\n",
    "text = \"game is on 🔥\"\n",
    "convert_emojis(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tagging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('sentence', 'NN'), ('written', 'VBN'), ('by', 'IN'), ('me', 'PRP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# Using nltk\n",
    "\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk import pos_tag \n",
    "  \n",
    "text = \"This is a sentence written by me.\"\n",
    "tags = pos_tag([token for token in word_tokenize(text)])\n",
    "\n",
    "print(tags)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, token.pos_ gives the POS tag when using spacy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking\n",
    "Chunking is the process of extracting phrases from unstructured text and more structure to it. It is also known as shallow parsing. It is done on top of Part of Speech tagging. It groups word into “chunks”, mainly of noun phrases. Chunking is done using regular expressions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP the/DT little/JJ yellow/JJ bird/NN)\n",
      "  is/VBZ\n",
      "  flying/VBG\n",
      "  in/IN\n",
      "  (NP the/DT sky/NN))\n",
      "(NP the/DT little/JJ yellow/JJ bird/NN)\n",
      "(NP the/DT sky/NN)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize  \n",
    "from nltk import pos_tag \n",
    "  \n",
    "# define chunking function with text and regular \n",
    "# expression representing grammar as parameter \n",
    "def chunking(text, grammar): \n",
    "    word_tokens = word_tokenize(text) \n",
    "  \n",
    "    # label words with part of speech \n",
    "    word_pos = pos_tag(word_tokens) \n",
    "  \n",
    "    # create a chunk parser using grammar \n",
    "    chunkParser = nltk.RegexpParser(grammar) \n",
    "  \n",
    "    # test it on the list of word tokens with tagged pos \n",
    "    tree = chunkParser.parse(word_pos) \n",
    "      \n",
    "    for subtree in tree.subtrees(): \n",
    "        print(subtree) \n",
    "      \n",
    "sentence = 'the little yellow bird is flying in the sky'\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "chunking(sentence, grammar) \n",
    "\n",
    "# Reference :https://www.geeksforgeeks.org/text-preprocessing-in-python-set-2/?ref=rp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition\n",
    "\n",
    "NER is used to extract information from unstructured text. It is used to classify entities present in a text into categories like a person, organization, event, places, etc. It gives us detailed knowledge about the text and the relationships between the different entities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Joe/NNP)\n",
      "  works/VBZ\n",
      "  for/IN\n",
      "  (ORGANIZATION Samsung/NNP)\n",
      "  so/RB\n",
      "  he/PRP\n",
      "  went/VBD\n",
      "  to/TO\n",
      "  (GPE Korea/NNP)\n",
      "  for/IN\n",
      "  a/DT\n",
      "  meetup/NN\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk import pos_tag, ne_chunk \n",
    "  \n",
    "def named_entity_recognition(text): \n",
    "    # tokenize the text \n",
    "    word_tokens = word_tokenize(text) \n",
    "  \n",
    "    # part of speech tagging of words \n",
    "    word_pos = pos_tag(word_tokens) \n",
    "  \n",
    "    # tree of word entities \n",
    "    print(ne_chunk(word_pos)) \n",
    "  \n",
    "text = 'Joe works for Samsung so he went to Korea for a meetup.'\n",
    "named_entity_recognition(text) \n",
    "\n",
    "# Reference :https://www.geeksforgeeks.org/text-preprocessing-in-python-set-2/?ref=rp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional References\n",
    "- https://stackabuse.com/python-for-nlp-tokenization-stemming-and-lemmatization-with-spacy-library/\n",
    "- https://spacy.io/\n",
    "- https://machinelearningmastery.com/clean-text-machine-learning-python/\n",
    "- https://www.kaggle.com/sudalairajkumar/getting-started-with-text-preprocessing \n",
    "- https://www.nltk.org/howto/stem.html\n",
    "- https://towardsdatascience.com/nlp-text-preprocessing-a-practical-guide-and-template-d80874676e79\n",
    "- https://medium.com/@datamonsters/text-preprocessing-in-python-steps-tools-and-examples-bf025f872908\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
